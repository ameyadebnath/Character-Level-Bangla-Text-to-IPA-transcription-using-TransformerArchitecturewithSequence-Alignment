{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "phoneme_training_notebook",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'dataverse_2023:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F61954%2F6762309%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240807%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240807T113002Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D6c1e319bffdf7dde461e3202a9e8a7fe8cbb60ee2d8df96043e4a4d6981a5fdd2d0d9de3026912bc3776e7081b3ec406740fa876b7f39df1373e7312c37a449fea49725fbe360663a644042c8e110938ab1a4c9433fb8e84d837bed3bba5864337df4149381d96640a4920deed71b4b37c6116b1acb8c476abf4d8019e1f5becac732c62f8c74c943cda87a3bab00b1c789939fe8c70fd3bb4da7586997bc7da51e76766355832d149e308a051921c83ec88fb710772da78782df4f4c87ef6a241e02c9cb0927eb547d30871e1a713e42603fc8f9cc1ccc154efcf7d8e23728ea8d2cace3f34c3d22996851e700e4f21e4b720101eeafb61e20809e1ad1f720e,text2ipa-mapping-trainset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F3928977%2F6833907%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240807%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240807T113002Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D69d6dd945f1c32b11ede814027731f5291104dc9f8e1a5d38f3f045039f9effca20d3d67cb101e402ecdf4d133c7b0832b7882092d930f31dd46441c4dc809b3f8a8285bb9c7133a3b58eea57514422f9227c28524c801069b9c527045628539424b189ef7df0506e5fd191981e685c819889d1138581f7398ad8b5d01495b070a96ac533085f57c8b2769c8ec26e5d00334a35eb3ddfe8b2244b56b76f354d956501c510cb9dd407ff76447865ac6b8379ac2ab1c951946bcc351fb0aacdb52b8fd06f92ce11d2fba9d60f1817fd7bd94149d494dd9363244142ad7d581e3f5862003515c3ee24de47de2b1042386b755314f48bbe9149c1e4dacbe683d7cc1'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "_yB9mk1ymqjf"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing libraries"
      ],
      "metadata": {
        "id": "LmBs-oZBmqjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import random\n",
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "random.seed(57)"
      ],
      "metadata": {
        "id": "bGyd11R9Gz0G",
        "execution": {
          "iopub.status.busy": "2023-10-30T09:20:35.517548Z",
          "iopub.execute_input": "2023-10-30T09:20:35.518152Z",
          "iopub.status.idle": "2023-10-30T09:20:49.708895Z",
          "shell.execute_reply.started": "2023-10-30T09:20:35.518117Z",
          "shell.execute_reply": "2023-10-30T09:20:49.707706Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading dataset"
      ],
      "metadata": {
        "id": "gxE8G8h5mqjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "path = \"/kaggle/input/text2ipa-mapping-trainset/previous_trainset_word_ipa_map_37807.csv\"\n",
        "df = pd.read_csv(path).drop('Unnamed: 0', axis=1)\n",
        "print(\"Length of training set\", len(df))\n",
        "df.head(5)"
      ],
      "metadata": {
        "id": "pyFtcaV3Gz0H",
        "execution": {
          "iopub.status.busy": "2023-10-30T09:21:51.044989Z",
          "iopub.execute_input": "2023-10-30T09:21:51.045461Z",
          "iopub.status.idle": "2023-10-30T09:21:51.131491Z",
          "shell.execute_reply.started": "2023-10-30T09:21:51.045428Z",
          "shell.execute_reply": "2023-10-30T09:21:51.130496Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notebook for creating word - ipa mapping:\n",
        "\n",
        "https://www.kaggle.com/code/jakir57/dataverse-eda-dictionary-creation"
      ],
      "metadata": {
        "id": "1oMEetTrmqjw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Converting words into sentences of characters"
      ],
      "metadata": {
        "id": "NhWdbL0kmqjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def word_sentence(word):\n",
        "  sentence = \"\"\n",
        "  for ch in word:\n",
        "    sentence += (ch + \" \")\n",
        "  return sentence\n",
        "\n",
        "text_pairs = []\n",
        "for index, row in df.iterrows():\n",
        "  word = word_sentence(row[\"word\"])\n",
        "  ipa = row[\"ipa\"]\n",
        "  ipa = \"[start] \" + word_sentence(ipa) + \"[end]\"\n",
        "  text_pairs.append((word, ipa))\n",
        "print(len(text_pairs), \"training pairs loaded..\")"
      ],
      "metadata": {
        "id": "kQeujujzGz0I",
        "execution": {
          "iopub.status.busy": "2023-10-30T09:24:51.855728Z",
          "iopub.execute_input": "2023-10-30T09:24:51.85609Z",
          "iopub.status.idle": "2023-10-30T09:24:54.476084Z",
          "shell.execute_reply.started": "2023-10-30T09:24:51.85606Z",
          "shell.execute_reply": "2023-10-30T09:24:54.475125Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sample training pairs"
      ],
      "metadata": {
        "id": "93Mjw4utmqjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(5):\n",
        "    print(random.choice(text_pairs))"
      ],
      "metadata": {
        "id": "1kelDUkrGz0J",
        "execution": {
          "iopub.status.busy": "2023-10-30T09:25:27.790405Z",
          "iopub.execute_input": "2023-10-30T09:25:27.790769Z",
          "iopub.status.idle": "2023-10-30T09:25:27.795862Z",
          "shell.execute_reply.started": "2023-10-30T09:25:27.79074Z",
          "shell.execute_reply": "2023-10-30T09:25:27.794958Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.01 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) -  num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples :]\n",
        "\n",
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")"
      ],
      "metadata": {
        "id": "H2HFz4B3Gz0J",
        "execution": {
          "iopub.status.busy": "2023-10-30T09:26:38.929336Z",
          "iopub.execute_input": "2023-10-30T09:26:38.929696Z",
          "iopub.status.idle": "2023-10-30T09:26:38.971149Z",
          "shell.execute_reply.started": "2023-10-30T09:26:38.929668Z",
          "shell.execute_reply": "2023-10-30T09:26:38.970172Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vocabulary creation and Text to vectorization"
      ],
      "metadata": {
        "id": "7PeaaprYmqj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequence_length = 64\n",
        "batch_size = 64\n",
        "\n",
        "# Defining vocabulary\n",
        "vb = ['', '[UNK]', '[start]', '[end]', 'া', 'র', '্', 'ে', 'ি', 'ন', 'ক', 'ব', 'স', 'ল', 'ত', 'ম', 'প', 'ু', 'দ', 'ট', 'য়', 'জ', '।', 'ো', 'গ', 'হ', 'য', 'শ', 'ী', 'ই', 'চ', 'ভ', 'আ', 'ও', 'ছ', 'ষ', 'ড', 'ফ', 'অ', 'ধ', 'খ', 'ড়', 'উ', 'ণ', 'এ', 'থ', 'ং', 'ঁ', 'ূ', 'ৃ', 'ঠ', 'ঘ', 'ঞ', 'ঙ', 'ৌ', '‘', 'ৎ', 'ঝ', 'ৈ', '়', 'ঢ', 'ঃ', 'ঈ', '\\u200c', 'ৗ', 'a', 'ঐ', 'd', 'w', 'ঋ', 'i', 'e', 't', 's', 'n', 'm', 'b', '“', 'u', 'r', 'œ', 'o', '–', 'ঊ', 'ঢ়', 'Í', 'g', 'p', '\\xad', 'h', 'c', 'l', 'ঔ', 'ƒ', '”', 'Ñ', '¡', 'y', 'j', 'f', '→', '—', 'ø', 'è', '¦', '¥', 'x', 'v', 'k']\n",
        "vipa = ['', '[UNK]', '[start]', '[end]', 'ɐ', 'ɾ', 'i', 'o', 'e', '̪', 't', 'n', 'k', 'ɔ', 'ʃ', 'b', 'd', 'l', 'u', 'p', 'm', 'ʰ', 'ɟ', '͡', '̯', 'g', 'ʱ', '।', 'c', 'ʲ', 'h', 's', 'ŋ', 'ɛ', 'ɽ', '̃', 'ʷ', '‘', '“', '–', '”', '—', 'w', 'j']\n",
        "\n",
        "v = vb + vipa\n",
        "s = set()\n",
        "for ch in v:\n",
        "  s.add(ch)\n",
        "\n",
        "vocab = sorted(list(s))\n",
        "print(\"Size of vocabulary is\", len(s))\n",
        "print(vocab)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "\n",
        "eng_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size, output_mode=\"int\", output_sequence_length=sequence_length,\n",
        "    vocabulary=vocab\n",
        ")\n",
        "spa_vectorization = TextVectorization(\n",
        "    max_tokens=vocab_size,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=sequence_length + 1,\n",
        "    vocabulary=vocab\n",
        ")\n",
        "train_eng_texts = [pair[0] for pair in text_pairs]\n",
        "train_spa_texts = [pair[1] for pair in text_pairs]\n",
        "\n",
        "# eng_vectorization.adapt(train_eng_texts)\n",
        "# spa_vectorization.adapt(train_spa_texts)"
      ],
      "metadata": {
        "id": "adChEXp5Gz0K",
        "execution": {
          "iopub.status.busy": "2023-10-30T09:31:23.629633Z",
          "iopub.execute_input": "2023-10-30T09:31:23.629995Z",
          "iopub.status.idle": "2023-10-30T09:31:23.671006Z",
          "shell.execute_reply.started": "2023-10-30T09:31:23.629966Z",
          "shell.execute_reply": "2023-10-30T09:31:23.670013Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating vectorized train and validation set"
      ],
      "metadata": {
        "id": "rOd2dF3Jmqj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_dataset(eng, spa):\n",
        "    eng = eng_vectorization(eng)\n",
        "    spa = spa_vectorization(spa)\n",
        "    return ({\"encoder_inputs\": eng, \"decoder_inputs\": spa[:, :-1],}, spa[:, 1:])\n",
        "\n",
        "def make_dataset(pairs):\n",
        "    eng_texts, spa_texts = zip(*pairs)\n",
        "    eng_texts = list(eng_texts)\n",
        "    spa_texts = list(spa_texts)\n",
        "    print(spa_texts)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.map(format_dataset)\n",
        "    return dataset.shuffle(2048).prefetch(16).cache()\n",
        "\n",
        "train_ds = make_dataset(train_pairs)\n",
        "val_ds = make_dataset(val_pairs)"
      ],
      "metadata": {
        "id": "0UpCtmt_Gz1M",
        "execution": {
          "iopub.status.busy": "2023-10-30T09:33:36.364266Z",
          "iopub.execute_input": "2023-10-30T09:33:36.365111Z",
          "iopub.status.idle": "2023-10-30T09:33:36.955504Z",
          "shell.execute_reply.started": "2023-10-30T09:33:36.365077Z",
          "shell.execute_reply": "2023-10-30T09:33:36.954478Z"
        },
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_ds.take(1):\n",
        "    print(f'inputs[\"encoder_inputs\"].shape: {inputs[\"encoder_inputs\"].shape}')\n",
        "    print(f'inputs[\"decoder_inputs\"].shape: {inputs[\"decoder_inputs\"].shape}')\n",
        "    print(f\"targets.shape: {targets.shape}\")"
      ],
      "metadata": {
        "id": "wxTAW1_pGz1N",
        "execution": {
          "iopub.status.busy": "2023-10-30T09:33:49.309979Z",
          "iopub.execute_input": "2023-10-30T09:33:49.310699Z",
          "iopub.status.idle": "2023-10-30T09:33:49.873682Z",
          "shell.execute_reply.started": "2023-10-30T09:33:49.310666Z",
          "shell.execute_reply": "2023-10-30T09:33:49.872771Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer model"
      ],
      "metadata": {
        "id": "bd_T-aiVmqj6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.dense_dim = dense_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "        attention_output = self.attention(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=padding_mask\n",
        "        )\n",
        "        proj_input = self.layernorm_1(inputs + attention_output)\n",
        "        proj_output = self.dense_proj(proj_input)\n",
        "        return self.layernorm_2(proj_input + proj_output)\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"dense_dim\": self.dense_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "\n",
        "class PositionalEmbedding(layers.Layer):\n",
        "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocab_size, output_dim=embed_dim\n",
        "        )\n",
        "        self.position_embeddings = layers.Embedding(\n",
        "            input_dim=sequence_length, output_dim=embed_dim\n",
        "        )\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def call(self, inputs):\n",
        "        length = tf.shape(inputs)[-1]\n",
        "        positions = tf.range(start=0, limit=length, delta=1)\n",
        "        embedded_tokens = self.token_embeddings(inputs)\n",
        "        embedded_positions = self.position_embeddings(positions)\n",
        "        return embedded_tokens + embedded_positions\n",
        "\n",
        "    def compute_mask(self, inputs, mask=None):\n",
        "        return tf.math.not_equal(inputs, 0)\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"sequence_length\": self.sequence_length,\n",
        "            \"vocab_size\": self.vocab_size,\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "\n",
        "class TransformerDecoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.attention_1 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.attention_2 = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim\n",
        "        )\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.layernorm_3 = layers.LayerNormalization()\n",
        "        self.supports_masking = True\n",
        "\n",
        "    def call(self, inputs, encoder_outputs, mask=None):\n",
        "        causal_mask = self.get_causal_attention_mask(inputs)\n",
        "        if mask is not None:\n",
        "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
        "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
        "\n",
        "        attention_output_1 = self.attention_1(\n",
        "            query=inputs, value=inputs, key=inputs, attention_mask=causal_mask\n",
        "        )\n",
        "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
        "\n",
        "        attention_output_2 = self.attention_2(\n",
        "            query=out_1,\n",
        "            value=encoder_outputs,\n",
        "            key=encoder_outputs,\n",
        "            attention_mask=padding_mask,\n",
        "        )\n",
        "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
        "\n",
        "        proj_output = self.dense_proj(out_2)\n",
        "        return self.layernorm_3(out_2 + proj_output)\n",
        "\n",
        "    def get_causal_attention_mask(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
        "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
        "        j = tf.range(sequence_length)\n",
        "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
        "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
        "        mult = tf.concat(\n",
        "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)],\n",
        "            axis=0,\n",
        "        )\n",
        "        return tf.tile(mask, mult)\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"embed_dim\": self.embed_dim,\n",
        "            \"latent_dim\": self.latent_dim,\n",
        "            \"num_heads\": self.num_heads,\n",
        "        })\n",
        "        return config\n"
      ],
      "metadata": {
        "id": "TVriAVGhGz1P",
        "execution": {
          "iopub.status.busy": "2023-10-30T09:34:21.019477Z",
          "iopub.execute_input": "2023-10-30T09:34:21.020128Z",
          "iopub.status.idle": "2023-10-30T09:34:21.043152Z",
          "shell.execute_reply.started": "2023-10-30T09:34:21.020094Z",
          "shell.execute_reply": "2023-10-30T09:34:21.042408Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 256\n",
        "latent_dim = 2048\n",
        "num_heads = 8\n",
        "\n",
        "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
        "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n",
        "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
        "\n",
        "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
        "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
        "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
        "x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
        "\n",
        "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
        "transformer = keras.Model(\n",
        "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
        ")"
      ],
      "metadata": {
        "id": "HVwtFdXsGz1Q",
        "execution": {
          "iopub.status.busy": "2023-10-30T09:34:35.79928Z",
          "iopub.execute_input": "2023-10-30T09:34:35.799631Z",
          "iopub.status.idle": "2023-10-30T09:34:36.949968Z",
          "shell.execute_reply.started": "2023-10-30T09:34:35.799602Z",
          "shell.execute_reply": "2023-10-30T09:34:36.949207Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "JDm802wvmqj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs =  50 # This should be at least 30 for convergence\n",
        "\n",
        "transformer.summary()\n",
        "transformer.compile(\n",
        "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
        ")\n",
        "history=transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)"
      ],
      "metadata": {
        "id": "BDyDAFFzGz1R",
        "execution": {
          "iopub.status.busy": "2023-10-30T09:35:01.903553Z",
          "iopub.execute_input": "2023-10-30T09:35:01.903946Z",
          "iopub.status.idle": "2023-10-30T10:21:01.737042Z",
          "shell.execute_reply.started": "2023-10-30T09:35:01.903915Z",
          "shell.execute_reply": "2023-10-30T10:21:01.736209Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Accuracy and loss plot"
      ],
      "metadata": {
        "id": "VqODkh3umqj8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title(\"Model Accuaracy\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend([\"train\",\"valiadtion\"],loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wJeS-mr4Gz1S",
        "execution": {
          "iopub.status.busy": "2023-10-30T10:24:06.880103Z",
          "iopub.execute_input": "2023-10-30T10:24:06.880977Z",
          "iopub.status.idle": "2023-10-30T10:24:07.290962Z",
          "shell.execute_reply.started": "2023-10-30T10:24:06.880936Z",
          "shell.execute_reply": "2023-10-30T10:24:07.290004Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title(\"Model Loss\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend([\"training loss\",\"validation loss\"],loc='upper left')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3-Er5e2oGz1T",
        "execution": {
          "iopub.status.busy": "2023-10-30T10:24:11.390972Z",
          "iopub.execute_input": "2023-10-30T10:24:11.391326Z",
          "iopub.status.idle": "2023-10-30T10:24:11.694166Z",
          "shell.execute_reply.started": "2023-10-30T10:24:11.391297Z",
          "shell.execute_reply": "2023-10-30T10:24:11.693227Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the model"
      ],
      "metadata": {
        "id": "LK_sbMHwmqj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(transformer, export_dir='/kaggle/working/text2ipa-transformer-model')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-10-30T10:30:29.364421Z",
          "iopub.execute_input": "2023-10-30T10:30:29.365378Z",
          "iopub.status.idle": "2023-10-30T10:30:38.507982Z",
          "shell.execute_reply.started": "2023-10-30T10:30:29.365335Z",
          "shell.execute_reply": "2023-10-30T10:30:38.507035Z"
        },
        "trusted": true,
        "id": "_b9U9H26mqj9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}